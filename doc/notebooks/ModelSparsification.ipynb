{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to sparsify a Pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model parameters count=84095008\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaForMaskedLM\n",
    "from pytorch_block_sparse.util import SparseModelPatcher\n",
    "import re\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=52_000,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")\n",
    "\n",
    "model = RobertaForMaskedLM(config=config).cuda()\n",
    "\n",
    "# =>84 million parameters\n",
    "print(f\"Initial model parameters count={model.num_parameters()}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta\\.encoder\\.layer\\.[0-9]+\\.attention\\.self\\.query\n",
      "roberta\\.encoder\\.layer\\.[0-9]+\\.attention\\.self\\.key\n",
      "roberta\\.encoder\\.layer\\.[0-9]+\\.attention\\.self\\.value\n",
      "roberta\\.encoder\\.layer\\.[0-9]+\\.attention\\.output\\.dense\n",
      "roberta\\.encoder\\.layer\\.[0-9]+\\.intermediate\\.dense\n",
      "roberta\\.encoder\\.layer\\.[0-9]+\\.output\\.dense\n",
      "roberta\\.pooler\\.dense\n",
      "lm_head\\.dense\n",
      "lm_head\\.decoder\n"
     ]
    }
   ],
   "source": [
    "# Create a model patcher\n",
    "mp = SparseModelPatcher()\n",
    "\n",
    "# Show names that can be used: this returns a list of all names in the network that are patchable.\n",
    "# These names are escaped to be used as regexps in mp.add_pattern()\n",
    "ret = mp.get_names(model)\n",
    "\n",
    "dedup_layers = []\n",
    "\n",
    "# Pretty print the regexps: replace layer number with regexp matching numbers, and dedup them\n",
    "for r in ret:\n",
    "    r = re.sub(r'[0-9]+', '[0-9]+', r)\n",
    "    if r not in dedup_layers:\n",
    "        dedup_layers.append(r)\n",
    "        print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patching 'roberta.encoder.layer.0.attention.output.dense' with density=0.5, in=768, out=768,bias=True \n",
      "Patching 'roberta.encoder.layer.0.intermediate.dense' with density=0.5, in=768, out=3072,bias=True \n",
      "Patching 'roberta.encoder.layer.0.output.dense' with density=0.5, in=3072, out=768,bias=True \n",
      "Patching 'roberta.encoder.layer.1.attention.output.dense' with density=0.5, in=768, out=768,bias=True \n",
      "Patching 'roberta.encoder.layer.1.intermediate.dense' with density=0.5, in=768, out=3072,bias=True \n",
      "Patching 'roberta.encoder.layer.1.output.dense' with density=0.5, in=3072, out=768,bias=True \n",
      "Patching 'roberta.encoder.layer.2.attention.output.dense' with density=0.5, in=768, out=768,bias=True \n",
      "Patching 'roberta.encoder.layer.2.intermediate.dense' with density=0.5, in=768, out=3072,bias=True \n",
      "Patching 'roberta.encoder.layer.2.output.dense' with density=0.5, in=3072, out=768,bias=True \n",
      "Patching 'roberta.encoder.layer.3.attention.output.dense' with density=0.5, in=768, out=768,bias=True \n",
      "Patching 'roberta.encoder.layer.3.intermediate.dense' with density=0.5, in=768, out=3072,bias=True \n",
      "Patching 'roberta.encoder.layer.3.output.dense' with density=0.5, in=3072, out=768,bias=True \n",
      "Patching 'roberta.encoder.layer.4.attention.output.dense' with density=0.5, in=768, out=768,bias=True \n",
      "Patching 'roberta.encoder.layer.4.intermediate.dense' with density=0.5, in=768, out=3072,bias=True \n",
      "Patching 'roberta.encoder.layer.4.output.dense' with density=0.5, in=3072, out=768,bias=True \n",
      "Patching 'roberta.encoder.layer.5.attention.output.dense' with density=0.5, in=768, out=768,bias=True \n",
      "Patching 'roberta.encoder.layer.5.intermediate.dense' with density=0.5, in=768, out=3072,bias=True \n",
      "Patching 'roberta.encoder.layer.5.output.dense' with density=0.5, in=3072, out=768,bias=True \n",
      "Final model parameters count=68169760\n"
     ]
    }
   ],
   "source": [
    "# Selecting some layers to sparsify.\n",
    "# This is the \"artful\" part, as some parts are more prone to be sparsified, other may impact model precision too much.\n",
    "\n",
    "# Match layers using regexp (we escape the ., just because, it's more correct, but it does not change anything here)\n",
    "# the [0-9]+ match any layer number.\n",
    "# We setup a density of 0.5 on these layers, you can test other layers / densities .\n",
    "mp.add_pattern(\"roberta\\.encoder\\.layer\\.[0-9]+\\.intermediate\\.dense\", {\"density\":0.5})\n",
    "mp.add_pattern(\"roberta\\.encoder\\.layer\\.[0-9]+\\.output\\.dense\", {\"density\":0.5})\n",
    "mp.add_pattern(\"roberta\\.encoder\\.layer\\.[0-9]+\\.attention\\.output\\.dense\", {\"density\":0.5})\n",
    "mp.patch_model(model)\n",
    "\n",
    "print(f\"Final model parameters count={model.num_parameters()}\")\n",
    "\n",
    "# => 68 million parameters instead of 84 million parameters (embeddings are taking a lof space in Roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
